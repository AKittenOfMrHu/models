import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
import pytorch_lightning as pl


class ModelWrapper(pl.LightningDataModule):
    def __init__(self, model, config, loss_func=F.binary_cross_entropy):
        self.model = model
        self.config = config
        self.loss_func = loss_func

    def forward(self, x):
        output_dict = self.model(x)
        return output_dict
    
    def training_step(self, batch, batch_idx):
        self.device_type = next(self.parameters()).device
        pred = self(batch["waveform"])
        loss = self.loss_func(pred, batch["target"])
        self.log("loss", loss, on_epoch=True, prog_bar=True)
        return loss
    
    def training_epoch_end(self, outputs):
        pass
    
    def evaluate_metric(pred, target):
        return None

    def validation_step(self, batch, batch_idx):
        pred = self(batch["waveform"])["mainoutput"]
        return [pred.detach(), batch["target"].detach()]

    def validation_epoch_end(self, validation_step_outputs):
        self.device_type = next(self.parameters()).device
        pred = torch.cat([d[0] for d in validation_step_outputs], dim=0)
        target = torch.cat([d[1] for d in validation_step_outputs], dim=0)

        if torch.cuda.device_count() > 1:
            gather_pred = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]
            gather_target = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]
            dist.barrier()

        if torch.cuda.device_count() > 1:
            dist.all_gather(gather_pred, pred)
            dist.all_gather(gather_target, target)
            if dist.get_rank() == 0:
                gather_pred = torch.cat(gather_pred, dim=0).cpu().numpy()
                gather_pred = torch.cat(gather_target, dim=0).cpu().numpy()
                metric_dict = self.evaluate_metric(gather_pred, gather_target)
            self.log("acc", metric_dict["acc"] * float(dist.get_world_size()), on_epoch=True, prog_bar=True, sync_dist=True)
            dist.barrier()
        else:
            gather_pred = pred.cpu().numpy()
            gather_target = target.cpu().numpy()
            metric_dict = self.evaluate_metric(gather_pred, gather_target)
            self.log("acc", metric_dict["acc"] * float(dist.get_world_size()), on_epoch=True, prog_bar=True, sync_dist=True)

    

    def test_step(self, batch, batch_idx):
        self.device_type = next(self.parameters()).device
        pred = self(batch['waveform'])
        return [pred.detach(), batch["target"].detach()]


    def test_epoch_end(self, test_step_outputs):
        self.device_type = next(self.parameters()).device
        pred = torch.cat([d[0] for d in test_step_outputs], dim=0)
        target = torch.cat([d[1] for d in test_step_outputs], dim=0)

        if torch.cuda.device_count() > 1:
            gather_pred = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]
            gather_target = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]
            dist.barrier()

        if torch.cuda.device_count() > 1:
            dist.all_gather(gather_pred, pred)
            dist.all_gather(gather_target, target)
            if dist.get_rank() == 0:
                gather_pred = torch.cat(gather_pred, dim=0).cpu().numpy()
                gather_pred = torch.cat(gather_target, dim=0).cpu().numpy()
                metric_dict = self.evaluate_metric(gather_pred, gather_target)
            self.log("acc", metric_dict["acc"] * float(dist.get_world_size()), on_epoch=True, prog_bar=True, sync_dist=True)
            dist.barrier()
        else:
            gather_pred = pred.cpu().numpy()
            gather_target = target.cpu().numpy()
            metric_dict = self.evaluate_metric(gather_pred, gather_target)
            self.log("acc", metric_dict["acc"] * float(dist.get_world_size()), on_epoch=True, prog_bar=True, sync_dist=True)

    def configure_optimizers(self):
        optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.parameters()),
            lr = self.config.learnin_rate,
            betas = (0.9, 0.99), eps = 1e-08, weight_decay = 0.05
        )
        def lr_lambda(epoch):
            if epoch < 3:
                lr_scale = self.config.lr_rate[epoch]
            else:
                lr_scale = max(self.config.lr_rate[0] * (0.98 ** epoch), 0.03)
            return lr_scale
        scheduler = optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lr_lambda
        )

        return [optimizer], [scheduler]
